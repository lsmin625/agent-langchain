{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 프롬프트 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI LLM 준비 \n",
    "* 환경 변수(`.env` 파일)에서 API Key 로딩\n",
    "* 개발 환경에서는 `gpt-4o-mini` 또는 `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"{os.getenv('OPENAI_API_KEY')[:9]}***\")\n",
    "\n",
    "# OpenAI LLM 준비\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 역할 기반 메시지 구성\n",
    "| 역할 (Role)       | 클래스명                              | 설명                                              |\n",
    "| --------------- | --------------------------------- | ----------------------------------------------- |\n",
    "| system          | `SystemMessage`                   | 모델의 행동 지침이나 문맥을 설정하는 초기 메시지                     |\n",
    "| user            | `HumanMessage`                    | 사용자의 입력을 나타냄 (사람이 입력한 메시지)                      |\n",
    "| assistant       | `AIMessage`                       | LLM의 응답을 나타냄                                    |\n",
    "| function/tool   | `FunctionMessage` / `ToolMessage` | 도구 실행 결과나 함수 응답을 나타냄 (OpenAI tool 사용 시)         |\n",
    "| generic (역할 없음) | `ChatMessage`                     | 역할을 문자열로 직접 지정하여 유연하게 사용 (예: \"critic\", \"coach\") |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"당신은 '명탐정 코난' 전문가입니다. 질문에 친절하고 정확하게 답해 주세요.\"),\n",
    "    HumanMessage(content=\"코난이 작아진 이유가 뭐죠?\")\n",
    "]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM으로 역할 기반 메시지 주고 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)\n",
    "\n",
    "# 응답 출력\n",
    "print(\">> 응답:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트(`ChatPromptTemplate`) 활용 메시지 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 '명탐정 코난' 전문가입니다. 질문에 친절하고 정확하게 답해 주세요.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt_messages = chat_template.format_messages(\n",
    "    question=\"블랙 조직의 리더는 누구인가요?\"\n",
    ")\n",
    "\n",
    "print(prompt_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM으로 프롬프트 메시지 주고 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt_messages)\n",
    "\n",
    "# 응답 출력\n",
    "print(\">> 응답:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** End of Documents **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-t0JhnSEV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
