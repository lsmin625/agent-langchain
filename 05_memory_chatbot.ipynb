{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메모리 기능을 통한 챗봇 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 OpenAI LLM 준비 \n",
    "* 환경 변수(`.env` 파일)에서 API Key 로딩\n",
    "* 개발 환경에서는 `gpt-4o-mini` 또는 `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"{open_api_key[:9]}***\")\n",
    "\n",
    "# OpenAI LLM 준비\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역할 기반 프롬프트 템플릿 정의\n",
    "- `system` 역할: LLM에게 '명탐정 코난 전문가' 역할 부여\n",
    "- `MessagesPlaceholder` : 프롬프트에서 memory가 삽입될 위치 지정\n",
    "- `human` 역할: 사용자 질문 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 '명탐정 코난' 전문가입니다. 대화의 흐름을 기억하며 친절하게 답변하세요.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리 설정\n",
    "메시지 기록 저장 방식 정의\n",
    "\n",
    "### `langchain.memory` 모듈의 핵심 클래스\n",
    "|주요 클래스|설명|\n",
    "|---|---|\n",
    "|`ChatMessageHistory`|(`현재 코드 방식`) 메모리 내(in-memory) 리스트에 메시지를 순서대로 저장하는 가장 단순한 형태|\n",
    "|`ConversationBufferMemory`|`ChatMessageHistory`를 감싸서, 대화 기록 전체를 하나의 긴 문자열(버퍼)로 만들어 프롬프트에 전달|\n",
    "|`ConversationBufferWindowMemory`|대화 기록 중, 가장 최근의 n개만 저장하고 나머지는 제외|\n",
    "|`ConversationSummaryMemory`|대화가 진행됨에 따라, LLM을 사용해 이전 대화 내용을 요약하며 저장|\n",
    "|`VectorStoreRetrieverMemory`|대화 내용을 벡터(숫자 배열)로 변환하여 DB에 저장하고, 현재 질문과 의미적으로 가장 관련성 높은 과거 대화를 검색|\n",
    "\n",
    "### 영구 저장소(Persistent Storage) 연동시\n",
    "|클래스 (from ...)|연동 DB|\n",
    "|---|---|\n",
    "|`RedisChatMessageHistory (langchain_community.chat_message_histories)`|Redis|\n",
    "|`SQLChatMessageHistory (langchain_community.chat_message_histories)`|PostgreSQL, MySQL, SQLite 등|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# 실제 서비스에서는 Redis나 DB 같은 영구 저장소를 사용\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 특정 사용자의 대화 기록을 가져오는 함수 정의\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    \"\"\"세션 ID에 해당하는 메시지 기록을 가져오거나 새로 생성합니다.\"\"\"\n",
    "\n",
    "    if session_id not in store:\n",
    "        # 새로운 빈 대화 기록(ChatMessageHistory) 저장\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL 체인 파이프라인 구성\n",
    "\n",
    "### `chain: Runnable = prompt | llm` (기본 체인 조립)\n",
    "- `Runnable`은 이 chain이 LangChain에서 실행 가능한 단위임을 나타내는 타입 힌트\n",
    "\n",
    "### `RunnableWithMessageHistory(...)` (체인에 기억력 결합)\n",
    "- `RunnableWithMessageHistory` 작동 방식\n",
    "    1. 기록 조회: session_id를 이용해 get_session_history 함수를 호출하여 해당 사용자의 과거 대화 기록을 조회\n",
    "    2. 입력 조립: 가져온 과거 기록을 history_messages_key(chat_history)에, 사용자의 새 질문을 input_messages_key(question)에 담아 prompt | llm 체인에 전달할 완벽한 입력값을 생성\n",
    "    3. 답변 생성: prompt | llm 체인이 이 입력값을 받아 AI의 답변을 생성\n",
    "    4. 기록 저장: 생성된 AI의 답변과 사용자의 질문을 다시 get_session_history가 반환한 객체에 추가하여 최신 대화 내용을 저장(업데이트)\n",
    "    5. 최종 반환: 생성된 AI의 답변만을 사용자에게 최종적으로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 기본 체인(Chain) 조립 - 파이프(|)) 연산자를 사용하여 프롬프트와 LLM을 연결\n",
    "chain: Runnable = prompt | llm\n",
    "\n",
    "# 체인에 기억력(메모리) 결합\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,                                # 기억력을 부여할 기본 체인\n",
    "    get_session_history,                  # 대화 기록을 가져올 함수 (3단계에서 만듦)\n",
    "    input_messages_key=\"question\",        # 사용자 질문을 식별할 이름\n",
    "    history_messages_key=\"chat_history\",  # 대화 기록을 식별할 이름\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체인과 UI 구성\n",
    "### Gradio와 LangChain 연결 함수\n",
    "- `history = history or []` : Gradio의 Chatbot 컴포넌트는 history가 비어있을 경우(None을 전달) 빈 리스트로 초기화\n",
    "- `result = chain_with_memory.invoke(..)` : 챗봇 엔진 체인을 호출하고 대화 기록을 사용할 수 있게 config 부분에 session_id를 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio와 LangChain을 연결할 '브릿지' 함수\n",
    "def chat_fn(user_input, history, session_id: str):\n",
    "    \"\"\"Gradio 인터페이스를 위한 채팅 함수\"\"\"\n",
    "\n",
    "    history = history or []\n",
    "\n",
    "    result = chain_with_memory.invoke(\n",
    "        {\"question\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    \n",
    "    # (사용자 입력, 챗봇 답변) 쌍을 history 리스트에 추가\n",
    "    history.append((user_input, result.content))\n",
    "    \n",
    "    # 업데이트된 history 리스트를 반환합니다. 이 값이 Chatbot 컴포넌트에 전달\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio의 세션 관리\n",
    "`session_id_state = gr.State(lambda: str(uuid.uuid4()))` 작동 원리\n",
    "1. `gr.State`의 초기화 규칙:\n",
    "    - gr.State의 value 인자로 **함수(Function)나 lambda**를 전달\n",
    "    - Gradio는 이 함수를 세션이 처음 시작될 때 단 한 번만 호출하여 그 반환 값을 초기값으로 사용\n",
    "2. `lambda`의 역할\n",
    "    - 여기서 lambda는 타입 힌트가 아니라, **이름 없는 일회용 함수(Anonymous Function)**\n",
    "    - lambda를 사용하는 이유는 코드 실행 시점을 제어 : `gr.State`는 함수 객체를 받아두었다가, 새로운 세션이 시작될 때마다 그 함수를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "\n",
    "# Gradio UI 레이아웃 설계\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"### 🕵️‍♂️ 명탐정 코난 전문가와 대화해보세요!\")\n",
    "    chatbot = gr.Chatbot(label=\"명탐정 코난 전문가 챗봇\", height=400)\n",
    "    txt = gr.Textbox(placeholder=\"예: 검은 조직의 보스는 누구야?\", show_label=False)\n",
    "    \n",
    "    # 사용자별 고유 ID를 저장하는 상태(State) 값\n",
    "    session_id_state = gr.State(lambda: str(uuid.uuid4()))\n",
    "\n",
    "    # '입력'과 '챗봇 엔진'과 '출력'을 연결\n",
    "    txt.submit(\n",
    "        chat_fn,  # Enter 키를 누르면 실행될 함수 (챗봇 로직)\n",
    "        inputs=[txt, chatbot, session_id_state], # chat_fn에 전달될 입력값들\n",
    "        outputs=[chatbot], # chat_fn이 반환한 결과로 업데이트될 출력 컴포넌트\n",
    "    )\n",
    "\n",
    "    # 입력창 초기화\n",
    "    txt.submit(lambda: \"\", None, txt)\n",
    "\n",
    "# 웹 서버 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** End of Documents **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-t0JhnSEV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
