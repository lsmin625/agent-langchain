{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìš©ì˜ì ëŒ€í™” ë…¹ìŒê¸° (ë©”ëª¨ë¦¬)\n",
    "\n",
    "- ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì •ë³´ëŠ” LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ë°›ì•„ ì˜¤ê³  UI ì±„íŒ…ì°½ì„ í†µí•´ ì¶œë ¥ëœë‹¤.\n",
    "- ì‚¬ìš©ìê°€ **\"ê¸°ë¡ ì‹œì‘\" , \"ë…¹ìŒ ì‹œì‘\"** ì§€ì‹œë¥¼ í•˜ë©´ ì´í›„ì˜ ëŒ€í™” ë‚´ìš©ì€ ê¸°ì–µí•˜ê³ , ê·¸ ì´ì „ì˜ ë‚´ìš©ì€ ê¸°ì–µí•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "- ì‚¬ìš©ìê°€ **\"ê¸°ë¡ ì¤‘ì§€\" , \"ë…¹ìŒ ì¤‘ì§€\"** ì§€ì‹œë¥¼ í•˜ë©´ ê·¸ ì´í›„ì˜ ë‚´ìš©ì€ ê¸°ì–µí•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "- ì‚¬ìš©ìê°€ **\"ê¸°ë¡ ì‚­ì œ\" , \"ë…¹ìŒ ì‚­ì œ\"** ì§€ì‹œë¥¼ í•˜ë©´ ëª¨ë“  ë‚´ìš©ì€ ì‚­ì œí•œë‹¤.\n",
    "- Gradio UIë¥¼ í†µí•´ ë…¹ìŒê¸°ì˜ ìƒíƒœë¥¼ í‘œì‹œí•œë‹¤.\n",
    "- memoryëŠ” í˜„ì¬ ì„¸ì…˜ì—ì„œë§Œ ìºì‹œ í˜•íƒœë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë„ë¡ ê´€ë¦¬í•œë‹¤.\n",
    "\n",
    "**â€» ëŒ€í™” ê¸°ë¡ì´ ìˆëŠ” ê²½ìš°, ê¸°ë¡ì´ ì—†ëŠ” ê²½ìš°ì— ëŒ€í™” ìš”ì•½ì„ ìš”ì²­í•˜ì—¬ ê²°ê³¼ë¥¼ ë¹„êµí•œë‹¤.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° OpenAI LLM ì¤€ë¹„ \n",
    "* í™˜ê²½ ë³€ìˆ˜(`.env` íŒŒì¼)ì—ì„œ API Key ë¡œë”©\n",
    "* ê°œë°œ í™˜ê²½ì—ì„œëŠ” `gpt-4o-mini` ë˜ëŠ” `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import List, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.tools import tool\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI LLM ì¤€ë¹„\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒíƒœ ê´€ë¦¬ (ë©”ëª¨ë¦¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState(BaseModel):\n",
    "    \"\"\"ëŒ€í™” ëª©ë¡ê³¼ ê¸°ë¡ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ëª¨ë¸\"\"\"\n",
    "    is_recording: bool = False\n",
    "    memory: List[HumanMessage | AIMessage] = Field(description=\"ì‚¬ìš©ìì™€ AIì˜ ëŒ€í™” ëª©ë¡\", default_factory=list)\n",
    "\n",
    "    def get_status(self) -> str:\n",
    "        \"\"\"í˜„ì¬ ê¸°ë¡ ìƒíƒœë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜\"\"\"\n",
    "        return \"ğŸ”´ ê¸°ë¡ ì¤‘\" if self.is_recording else \"âš«ï¸ ê¸°ë¡ ëŒ€ê¸°\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë©”ëª¨ë¦¬ ê´€ë¦¬ ë„êµ¬ (tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def start_memory_recording(state: AppState) -> str:\n",
    "    \"\"\"ì‚¬ìš©ìê°€ ëŒ€í™” ê¸°ë¡ ì‹œì‘ì„ ìš”ì²­í•  ë•Œ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 'ê¸°ë¡ ì‹œì‘', 'ë…¹ìŒ ì‹œì‘' ë“±ì˜ ëª…ë ¹ì–´ì— í•´ë‹¹í•©ë‹ˆë‹¤.\"\"\"\n",
    "    state.is_recording = True\n",
    "    return \"ì§€ê¸ˆë¶€í„° ëŒ€í™”ë¥¼ ê¸°ë¡í•˜ê¸° ì‹œì‘í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "@tool\n",
    "def stop_memory_recording(state: AppState) -> str:\n",
    "    \"\"\"ì‚¬ìš©ìê°€ ëŒ€í™” ê¸°ë¡ ì¤‘ì§€ë¥¼ ìš”ì²­í•  ë•Œ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 'ê¸°ë¡ ì¤‘ì§€', 'ë…¹ìŒ ì¤‘ì§€' ë“±ì˜ ëª…ë ¹ì–´ì— í•´ë‹¹í•©ë‹ˆë‹¤.\"\"\"\n",
    "    state.is_recording = False\n",
    "    return \"ì§€ê¸ˆë¶€í„° ëŒ€í™”ë¥¼ ê¸°ë¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "@tool\n",
    "def clear_all_memory(state: AppState) -> str:\n",
    "    \"\"\"ì‚¬ìš©ìê°€ ëª¨ë“  ëŒ€í™” ê¸°ë¡ ì‚­ì œë¥¼ ìš”ì²­í•  ë•Œ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 'ê¸°ë¡ ì‚­ì œ', 'ë…¹ìŒ ì‚­ì œ' ë“±ì˜ ëª…ë ¹ì–´ì— í•´ë‹¹í•©ë‹ˆë‹¤.\"\"\"\n",
    "    state.memory.clear()\n",
    "    return \"ëª¨ë“  ëŒ€í™” ê¸°ë¡ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "1. `system_prompt`: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿(`ChatPromptTemplate`)ì— ê³ ì •ì ìœ¼ë¡œ í¬í•¨ëœ ë¶€ë¶„ - AIì˜ ì—­í• ì´ë‚˜ í–‰ë™ ì§€ì¹¨ì„ ì •ì˜í•˜ëŠ” ì •ì ì¸ í…ìŠ¤íŠ¸ë¡œ, ëŒ€í™”ê°€ ì§„í–‰ë˜ì–´ë„ ë³€í•˜ì§€ ì•ŠìŒ.\n",
    "\n",
    "2. chat_history (state.memory): `MessagesPlaceholderë¥¼` í†µí•´ ë™ì ìœ¼ë¡œ ì±„ì›Œì§€ëŠ” ëŒ€í™” ê¸°ë¡ - `clear_all_memory` í•¨ìˆ˜ëŠ” ë°”ë¡œ ì´ state.memory ë¦¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ë§Œ ì‚­ì œ.\n",
    "\n",
    "* ë”°ë¼ì„œ 'ê¸°ë¡ ì‚­ì œ' ëª…ë ¹ì„ ì‹¤í–‰í•˜ë©´ state.memory ë¦¬ìŠ¤íŠ¸ëŠ” ë¹„ì›Œì§€ì§€ë§Œ, system_promptëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì¡°ì˜ ì¼ë¶€ë¡œì„œ ê·¸ëŒ€ë¡œ ìœ ì§€ - ì´í›„ ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•  ë•Œë„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” í•­ìƒ LLMì— ì „ë‹¬ë¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ëª…íƒì • ì½”ë‚œì˜ ìˆ˜ì‚¬í•˜ëŠ” ì‚¬ê±´ì˜ ìœ ë ¥í•œ ìš©ì˜ì ì…ë‹ˆë‹¤.\n",
    "íƒì •ì˜ ì§ˆë¬¸ì— ì ì ˆí•˜ê²Œ ë‹µë³€ì„ í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "íƒì •ì€ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ë…¹ìŒê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.: 'start_memory_recording', 'stop_memory_recording', 'clear_all_memory'.\n",
    "ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•´ì„œ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í—ˆìš©í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ 'ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤.' ë¼ê³  ë§í•˜ë©´ 'start_memory_recording' ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ê·¸ ì™¸ ëª¨ë“  ì¼ë°˜ì ì¸ ëŒ€í™”ì—ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  ì§ì ‘ ë‹µë³€í•´ì•¼ í•˜ê³  íƒì •ì—ê²Œ ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤..\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL íŒŒì´í”„ë¼ì¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [start_memory_recording, stop_memory_recording, clear_all_memory]\n",
    "\n",
    "# ë„êµ¬ ì´ë¦„ì„ í‚¤ë¡œ, í•¨ìˆ˜ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±\n",
    "tool_map = {t.name: t for t in tools}\n",
    "\n",
    "# llmì— ë„êµ¬ë“¤ì„ ë°”ì¸ë”©\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# LCEL ì²´ì¸ì„ êµ¬ì„±\n",
    "chain = prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_fn(user_message: str, history: list, state: AppState):\n",
    "    response = chain.invoke({\"input\": user_message, \"chat_history\": state.memory})\n",
    "\n",
    "    ai_message = \"\"\n",
    "    # LLM ì‘ë‹µì´ ë„êµ¬ í˜¸ì¶œì¼ ê²½ìš°\n",
    "    if response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            if tool_name in tool_map:\n",
    "                tool_to_call = tool_map[tool_name]\n",
    "                ai_message = tool_to_call.func(state=state)\n",
    "    else:\n",
    "        # ì¼ë°˜ ë©”ì‹œì§€ì¼ ê²½ìš°\n",
    "        ai_message = response.content\n",
    "\n",
    "    # historyëŠ” UI í‘œì‹œìš©, state.memoryëŠ” LLM ì „ë‹¬ìš©\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "\n",
    "    # ê¸°ë¡ ìƒíƒœì¼ ë•Œë§Œ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ëŒ€í™” ë‚´ìš© ì¶”ê°€\n",
    "    if state.is_recording:\n",
    "        # ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ì¼ë°˜ ëŒ€í™”ë§Œ ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "        if not response.tool_calls:\n",
    "            state.memory.extend(\n",
    "                [HumanMessage(content=user_message), AIMessage(content=ai_message)]\n",
    "            )\n",
    "\n",
    "    return history, state, state.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI ë ˆì´ì•„ì›ƒ ì„¤ê³„\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    initial_state = AppState()\n",
    "    state = gr.State(value=initial_state)\n",
    "\n",
    "    gr.Markdown(\"### ğŸ•µï¸â€â™‚ï¸ íƒì •ì˜ ë…¹ìŒê¸°!\")\n",
    "    gr.Markdown(\"`ê¸°ë¡ ì‹œì‘`, `ê¸°ë¡ ì¤‘ì§€`, `ê¸°ë¡ ì‚­ì œ` ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        status_indicator = gr.Textbox(\n",
    "            value=initial_state.get_status(),\n",
    "            label=\"ë©”ëª¨ë¦¬ ìƒíƒœ\",\n",
    "            interactive=False,\n",
    "        )\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"ëŒ€í™”ì°½\", height=300, type=\"messages\")\n",
    "\n",
    "    txt = gr.Textbox(placeholder=\"ì–´ì œ ì €ë… 9ì‹œì— ì–´ë””ì— ê³„ì…¨ì£ ?\", show_label=False)\n",
    "    txt.submit(\n",
    "        chat_fn,\n",
    "        inputs=[txt, chatbot, state],\n",
    "        outputs=[chatbot, state, status_indicator]\n",
    "    )\n",
    "\n",
    "    # ì…ë ¥ì°½ ì´ˆê¸°í™”\n",
    "    txt.submit(lambda: \"\", None, txt)\n",
    "\n",
    "# ì›¹ ì„œë²„ ì‹¤í–‰\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** End of Documents **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-t0JhnSEV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
