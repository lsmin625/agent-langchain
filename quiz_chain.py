import gradio as gr
import random, json, os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

QUIZ_FILE = "conan_quiz.json"
QUIZ_COUNT = 3

load_dotenv()
llm = ChatOpenAI(model="gpt-4o", temperature=0.7)


# í€´ì¦ˆ ë¡œë”©
def load_quiz():
    with open(QUIZ_FILE, "r", encoding="utf-8") as f:
        all_q = json.load(f)
    return random.sample(all_q, QUIZ_COUNT)


# ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶œë ¥
def get_question(state):
    idx = state["quiz_index"]
    q = state["questions"][idx]
    text = f"ë¬¸ì œ {idx+1}: {q['question']}"
    if q["type"] == "multiple_choice":
        choices = [f"{i+1}. {c}" for i, c in enumerate(q["choices"])]
        text += "\n" + "\n".join(choices)
    return text


# ì‚¬ìš©ì ë‹µë³€ì„ ìƒíƒœì— ì €ì¥
def update_state(state, user_input):
    idx = state["quiz_index"]
    q = state["questions"][idx]
    processed = user_input.strip()

    if q["type"] == "multiple_choice":
        try:
            sel = int(processed) - 1
            if 0 <= sel < len(q["choices"]):
                processed = q["choices"][sel]
        except:
            pass

    state["user_answers"].append(
        {
            "question_text": q["question"],
            "user_response": processed,
            "is_correct": False,
            "correct_answer": str(q["answer"]),
        }
    )
    state["quiz_index"] += 1
    return state


# ì±„ì ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
def build_grading_prompt(state):
    parts = [
        "ë‹¹ì‹ ì€ í€´ì¦ˆ ì±„ì ê´€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë‹µë³€ì„ ì •ë‹µ ì—¬ë¶€ë¡œ íŒë‹¨í•˜ê³  ê° ë¬¸ì œì— í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
        "ë§ˆì§€ë§‰ì—ëŠ” 'ì´ì : X/Y' í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.",
    ]
    for i, (q, a) in enumerate(zip(state["questions"], state["user_answers"])):
        parts.append(f"\në¬¸ì œ {i+1}: {q['question']}")
        if q["type"] == "multiple_choice":
            parts.append(f"ì„ íƒì§€: {', '.join(q['choices'])}")
        parts.append(f"ì •ë‹µ: {q['answer']}")
        parts.append(f"ì‚¬ìš©ì ë‹µë³€: {a['user_response']}")
    return "\n".join(parts)


# LCEL ì±„ì  ì²´ì¸
grade_chain = (
    ChatPromptTemplate.from_messages(
        [
            ("system", "ì±„ì ê´€ìœ¼ë¡œì„œ ì •ë‹µ íŒë‹¨ ë° í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”."),
            ("user", "{grading_input}"),
        ]
    )
    | llm
    | StrOutputParser()
)


# í€´ì¦ˆ ì‹œì‘ ìš”ì²­ ì²˜ë¦¬
def handle_quiz_start(user_input, quiz_state, messages):
    quiz_state["questions"] = load_quiz()
    quiz_state["quiz_index"] = 0
    quiz_state["user_answers"] = []
    qtext = get_question(quiz_state)
    messages.append([user_input, qtext])
    return quiz_state, messages


# í€´ì¦ˆê°€ ì´ë¯¸ ëë‚œ ê²½ìš°
def handle_quiz_already_done(user_input, messages):
    messages.append(
        [
            user_input,
            "í€´ì¦ˆê°€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´ 'í€´ì¦ˆ ì‹œì‘'ì´ë¼ê³  ì…ë ¥í•˜ì„¸ìš”.",
        ]
    )
    return messages


# ì‚¬ìš©ì ë‹µë³€ ì²˜ë¦¬
def handle_user_answer(user_input, quiz_state, messages):
    quiz_state = update_state(quiz_state, user_input)

    if quiz_state["quiz_index"] < len(quiz_state["questions"]):
        qtext = get_question(quiz_state)
        messages.append([user_input, qtext])
    else:
        prompt = build_grading_prompt(quiz_state)
        result = grade_chain.invoke({"grading_input": prompt})
        messages.append([user_input, result])

    return quiz_state, messages


# ë©”ì¸ ì±—ë´‡ ì²˜ë¦¬ í•¨ìˆ˜
def chat_fn(user_input, state):
    user_input_lower = user_input.strip().lower()
    messages = state["chat_history"]
    quiz_state = state["quiz_state"]

    if quiz_state["questions"] == []:
        if user_input_lower in ["í€´ì¦ˆ", "í€´ì¦ˆ ì‹œì‘"]:
            quiz_state, messages = handle_quiz_start(user_input, quiz_state, messages)
        else:
            messages.append(
                [
                    user_input,
                    "'í€´ì¦ˆ' ë˜ëŠ” 'í€´ì¦ˆ ì‹œì‘'ì´ë¼ê³  ì…ë ¥í•˜ë©´ í€´ì¦ˆë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
                ]
            )
    elif quiz_state["quiz_index"] >= len(quiz_state["questions"]):
        messages = handle_quiz_already_done(user_input, messages)
    else:
        quiz_state, messages = handle_user_answer(user_input, quiz_state, messages)

    state["quiz_state"] = quiz_state
    state["chat_history"] = messages
    return messages, state


# ìƒíƒœ ì´ˆê¸°í™”
def init_state():
    return {
        "quiz_state": {"quiz_index": 0, "questions": [], "user_answers": []},
        "chat_history": [],
    }


# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ•µï¸ ëª…íƒì • ì½”ë‚œ ë§¤ë‹ˆì•„ íŒë³„ê¸°")
    chatbot = gr.Chatbot(label="ëª…íƒì • ì½”ë‚œ í€´ì¦ˆ ì±—ë´‡", height=400)
    txt = gr.Textbox(placeholder="'í€´ì¦ˆ ì‹œì‘'ì„ ì…ë ¥í•´ë³´ì„¸ìš”!", show_label=False)
    state = gr.State(init_state())

    txt.submit(chat_fn, inputs=[txt, state], outputs=[chatbot, state])
    txt.submit(lambda: "", None, txt)

demo.launch()
