import gradio as gr
import random
import json
import os
from dotenv import load_dotenv
from typing import List, TypedDict

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langgraph.graph import StateGraph, END

# --- ì´ˆê¸° ì„¤ì • ë° ë°ì´í„° ë¡œë”© ---

QUIZ_FILE = "data/conan_quiz.json"
QUIZ_COUNT = 3

load_dotenv()
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)


# í€´ì¦ˆ ë¡œë”© í•¨ìˆ˜
def load_quiz():
    with open(QUIZ_FILE, "r", encoding="utf-8") as f:
        all_q = json.load(f)
    return random.sample(all_q, QUIZ_COUNT)


# --- Pydantic ëª¨ë¸ ë° State ì •ì˜ ---


class GradingResult(BaseModel):
    question: str = Field(description="ì±„ì  ëŒ€ìƒ ë¬¸ì œ")
    correct_answer: str = Field(description="ë¬¸ì œì˜ ì •ë‹µ")
    user_answer: str = Field(description="ì‚¬ìš©ìê°€ ì œì¶œí•œ ë‹µë³€")
    is_correct: bool = Field(description="ì •ë‹µ ì—¬ë¶€")
    explanation: str = Field(description="ì •ë‹µì— ëŒ€í•œ ì¹œì ˆí•œ í•´ì„¤")


class FinalReport(BaseModel):
    results: List[GradingResult] = Field(description="ê° ë¬¸ì œë³„ ì±„ì  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    total_score: str = Field(description="'ì´ì : X/Y' í˜•ì‹ì˜ ìµœì¢… ì ìˆ˜ ìš”ì•½")


class QuizState(TypedDict):
    user_input: str
    questions: List[dict]
    user_answers: List[str]
    quiz_index: int
    chat_history: List[tuple]
    grading_input_str: str | None
    final_report: FinalReport | None


# --- StateGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---


def start_quiz(state: QuizState) -> QuizState:
    """í€´ì¦ˆë¥¼ ì‹œì‘í•˜ê³  ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    questions = load_quiz()
    if not questions:
        state["chat_history"].append(
            ("assistant", "í€´ì¦ˆë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆê±°ë‚˜ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
        )
        state["questions"] = []
        return state

    state["questions"] = questions
    state["quiz_index"] = 0
    state["user_answers"] = []
    state["final_report"] = None
    state["chat_history"].append(("assistant", "ëª…íƒì • ì½”ë‚œ í€´ì¦ˆë¥¼ ì‹œì‘í•©ë‹ˆë‹¤! ğŸ•µï¸â€â™‚ï¸"))
    return state


def ask_question(state: QuizState) -> QuizState:
    """í˜„ì¬ quiz_indexì— ë§ëŠ” ë¬¸ì œë¥¼ í¬ë§·í•˜ì—¬ chat_historyì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    idx = state["quiz_index"]
    q = state["questions"][idx]

    text = f"ë¬¸ì œ {idx + 1}: {q['question']}"
    if q["type"] == "multiple_choice":
        choices = [f"{i + 1}. {c}" for i, c in enumerate(q["choices"])]
        text += "\n" + "\n".join(choices)

    state["chat_history"].append(("assistant", text))
    return state


def process_and_store_answer(state: QuizState) -> QuizState:
    """ì‚¬ìš©ì ë‹µë³€ì„ ì²˜ë¦¬í•˜ê³  ì €ì¥í•œ ë’¤, ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤."""
    idx = state["quiz_index"]
    q = state["questions"][idx]
    user_input = state["user_input"].strip()

    processed_answer = user_input
    if q["type"] == "multiple_choice":
        try:
            sel = int(user_input) - 1
            if 0 <= sel < len(q["choices"]):
                processed_answer = q["choices"][sel]
        except (ValueError, IndexError):
            pass

    state["user_answers"].append(processed_answer)
    state["quiz_index"] += 1
    return state


def prepare_grading_prompt(state: QuizState) -> QuizState:
    """ì±„ì ì„ ìœ„í•´ LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    state["chat_history"].append(
        (
            "assistant",
            "ëª¨ë“  ë¬¸ì œë¥¼ ë‹¤ í‘¸ì…¨êµ°ìš”! ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì‹œë©´ ì±„ì í•´ ë“œë¦´ê²Œìš”... ğŸ“",
        )
    )
    parts = [
        "ì, ì´ì œ ì•„ë˜ì˜ ë¬¸ì œì™€ ì •ë‹µ, ê·¸ë¦¬ê³  ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë³´ê³  ì±„ì ì„ ì‹œì‘í•´ì£¼ì„¸ìš”."
    ]
    for i, (q, a) in enumerate(zip(state["questions"], state["user_answers"])):
        parts.append(f"\n--- ë¬¸ì œ {i + 1} ---")
        parts.append(f"ë¬¸ì œ: {q['question']}")
        if q["type"] == "multiple_choice":
            parts.append(f"ì„ íƒì§€: {', '.join(q['choices'])}")
        parts.append(f"ì •ë‹µ: {q['answer']}")
        parts.append(f"ì‚¬ìš©ì ë‹µë³€: {a}")

    state["grading_input_str"] = "\n".join(parts)
    return state


def grade_with_llm_and_parse(state: QuizState) -> QuizState:
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ì±„ì í•˜ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = PydanticOutputParser(pydantic_object=FinalReport)
    system_message = "ë‹¹ì‹ ì€ 'ëª…íƒì • ì½”ë‚œ' í€´ì¦ˆì˜ ì „ë¬¸ ì±„ì ê´€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì œ, ì •ë‹µ, ì‚¬ìš©ì ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì±„ì í•´ì£¼ì„¸ìš”. ê° ë¬¸ì œì— ëŒ€í•´ ì •ë‹µ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³  ì¹œì ˆí•œ í•´ì„¤ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”. ëª¨ë“  ì±„ì ì´ ëë‚˜ë©´, ë§ˆì§€ë§‰ì—ëŠ” 'ì´ì : X/Y' í˜•ì‹ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ë¥¼ ë°˜ë“œì‹œ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤˜ì•¼ í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_message),
            ("human", "{grading_data}\n\n{format_instructions}"),
        ]
    )

    try:
        chain_input = {
            "grading_data": state["grading_input_str"],
            "format_instructions": parser.get_format_instructions(),
        }
        formatted_prompt = prompt.format_prompt(**chain_input)
        response = llm.invoke(formatted_prompt)
        report = parser.invoke(response)
        state["final_report"] = report
    except Exception as e:
        print(f"ì±„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        error_report = FinalReport(results=[], total_score="ì±„ì  ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        state["final_report"] = error_report

    return state


def format_final_report(state: QuizState) -> QuizState:
    """íŒŒì‹±ëœ ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    final_report_obj = state["final_report"]
    report_parts = ["ì±„ì ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰\n"]

    if final_report_obj and final_report_obj.results:
        for i, res in enumerate(final_report_obj.results):
            is_correct_text = "âœ… ì •ë‹µ" if res.is_correct else "âŒ ì˜¤ë‹µ"
            report_parts.append(f"--- ë¬¸ì œ {i + 1} ---")
            report_parts.append(f"ë¬¸ì œ: {res.question}")
            report_parts.append(f"ì •ë‹µ: {res.correct_answer}")
            report_parts.append(f"ì œì¶œí•œ ë‹µë³€: {res.user_answer}")
            report_parts.append(f"ê²°ê³¼: {is_correct_text}")
            report_parts.append(f"í•´ì„¤: {res.explanation}\n")
        report_parts.append(f"**{final_report_obj.total_score}**")
    else:
        report_parts.append("ì±„ì  ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    report_parts.append("\ní€´ì¦ˆë¥¼ ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´ 'í€´ì¦ˆ ì‹œì‘'ì´ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”.")
    state["chat_history"].append(("assistant", "\n".join(report_parts)))
    return state


def handle_invalid_start(state: QuizState) -> QuizState:
    """í€´ì¦ˆ ì‹œì‘ ëª…ë ¹ì–´ê°€ ì•„ë‹ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    bot_message = "'í€´ì¦ˆ' ë˜ëŠ” 'í€´ì¦ˆ ì‹œì‘'ì´ë¼ê³  ì…ë ¥í•˜ë©´ í€´ì¦ˆê°€ ì‹œì‘ë©ë‹ˆë‹¤."
    state["chat_history"].append(("assistant", bot_message))
    return state


# --- StateGraph ì¡°ê±´ë¶€ í•¨ìˆ˜ ---


def should_continue_quiz(state: QuizState) -> str:
    """í€´ì¦ˆë¥¼ ê³„ì†í• ì§€, ì±„ì ì„ ì‹œì‘í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if state["quiz_index"] < len(state["questions"]):
        return "continue_quiz"
    else:
        return "grade_quiz"


def route_initial_input(state: QuizState) -> str:
    """ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ì˜ ì‹œì‘ì ì„ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.get("questions") and state["questions"]:
        return "process_answer"
    else:
        if state["user_input"].strip().lower() in ["í€´ì¦ˆ", "í€´ì¦ˆ ì‹œì‘"]:
            return "start_quiz"
        else:
            return "invalid_start"


# --- StateGraph ì •ì˜ ë° ì»´íŒŒì¼ ---

workflow = StateGraph(QuizState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("start_quiz", start_quiz)
workflow.add_node("ask_question", ask_question)
workflow.add_node("process_answer", process_and_store_answer)
workflow.add_node("prepare_grading", prepare_grading_prompt)
workflow.add_node("grade_and_parse", grade_with_llm_and_parse)
workflow.add_node("format_report", format_final_report)
workflow.add_node("invalid_start", handle_invalid_start)

# === ì˜¤ë¥˜ ìˆ˜ì •: ì¡°ê±´ë¶€ ì§„ì…ì  ì„¤ì • ===
workflow.set_conditional_entry_point(
    route_initial_input,
    {
        "start_quiz": "start_quiz",
        "process_answer": "process_answer",
        "invalid_start": "invalid_start",
    },
)

# ì—£ì§€ ì—°ê²°
workflow.add_edge("start_quiz", "ask_question")
workflow.add_edge("ask_question", END)
workflow.add_edge("invalid_start", END)

workflow.add_conditional_edges(
    "process_answer",
    should_continue_quiz,
    {"continue_quiz": "ask_question", "grade_quiz": "prepare_grading"},
)
workflow.add_edge("prepare_grading", "grade_and_parse")
workflow.add_edge("grade_and_parse", "format_report")
workflow.add_edge("format_report", END)

quiz_app = workflow.compile()


# --- Gradio UI ë° ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜ ---


def init_state():
    return {"quiz_state": {"questions": [], "chat_history": []}}


def chat_fn(user_input, state):
    quiz_state = state["quiz_state"]

    if quiz_state.get("final_report") and user_input.strip().lower() in [
        "í€´ì¦ˆ",
        "í€´ì¦ˆ ì‹œì‘",
    ]:
        quiz_state = init_state()["quiz_state"]

    current_chat_history = quiz_state.get("chat_history", [])
    current_chat_history.append(("user", user_input))

    graph_input = {
        **quiz_state,
        "user_input": user_input,
        "chat_history": current_chat_history,
    }

    new_state = quiz_app.invoke(graph_input)

    state["quiz_state"] = new_state

    chat_display = [
        {"role": role, "content": content}
        for role, content in new_state["chat_history"]
    ]

    return chat_display, state


# --- UI ì •ì˜ ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("### ğŸ•µï¸ ëª…íƒì • ì½”ë‚œ ë§¤ë‹ˆì•„ íŒë³„ê¸° (LangGraph ver.)")

    chatbot = gr.Chatbot(
        label="ëª…íƒì • ì½”ë‚œ í€´ì¦ˆ ì±—ë´‡",
        height=500,
        avatar_images=("data/avatar_user.png", "data/avatar_conan.png"),
        type="messages",
    )

    txt = gr.Textbox(
        placeholder="'í€´ì¦ˆ ì‹œì‘'ì„ ì…ë ¥í•´ë³´ì„¸ìš”!", show_label=False, container=False
    )
    state = gr.State(init_state())
    txt.submit(chat_fn, inputs=[txt, state], outputs=[chatbot, state])
    txt.submit(lambda: "", None, txt, queue=False)

demo.launch(debug=True)
